# LangGraph AI Agents Developer Manual

## Table of Contents
1. [Introduction to LangGraph and AI Agents](#introduction-to-langgraph-and-ai-agents)
2. [Core Components Deep Dive](#core-components-deep-dive)
3. [Building Your First Agent](#building-your-first-agent)
4. [Advanced Agent Patterns](#advanced-agent-patterns)
5. [Multi-Action Execution](#multi-action-execution)
6. [Production Considerations](#production-considerations)

---

## Introduction to LangGraph and AI Agents

### What is LangGraph?

LangGraph is a library for building stateful, multi-actor applications with Large Language Models (LLMs). Unlike simple chat applications, LangGraph agents can:

- **Execute multiple actions** before providing a final response
- **Maintain state** across different steps of execution
- **Make decisions** about what actions to take next
- **Handle complex workflows** that require multiple tool calls and reasoning steps

### The Agent Paradigm vs. Traditional Chat

```python
# Traditional Chat: One question ‚Üí One response
user_question = "What's the weather in SF?"
response = llm.invoke(user_question)  # Single API call
print(response)

# Agent with LangGraph: One question ‚Üí Multiple actions ‚Üí Comprehensive response
user_question = "What's the weather in SF and how does it compare to LA?"
# Agent will:
# 1. Search for SF weather
# 2. Search for LA weather  
# 3. Compare the results
# 4. Provide comprehensive answer
```

### Why Agents Are Powerful

Agents can handle complex, multi-step tasks that require:

1. **Sequential reasoning**: "First I need to find X, then use that to determine Y"
2. **Parallel execution**: "I can search for multiple pieces of information simultaneously"  
3. **Error recovery**: "If this tool fails, I'll try a different approach"
4. **Context awareness**: "Based on what I found, I need to ask a follow-up question"

---

## Core Components Deep Dive

### Understanding the State

The state is the "memory" of your agent - it persists information across all nodes and edges in the graph.

```python
from typing import TypedDict, Annotated
import operator
from langchain_core.messages import AnyMessage

class AgentState(TypedDict):
    """
    Defines the state structure for our agent.
    
    The state acts as a shared memory that all nodes can read from and write to.
    This allows information to flow through the entire agent workflow.
    """
    
    # messages: A list that accumulates all conversation messages
    # The Annotated[list[AnyMessage], operator.add] means:
    # - Type: list of AnyMessage objects
    # - Reducer: operator.add (new messages are appended to existing ones)
    messages: Annotated[list[AnyMessage], operator.add]

# Why use operator.add as a reducer?
# When a node returns {'messages': [new_message]}, 
# LangGraph will automatically append new_message to the existing messages list
# instead of replacing the entire list
```

#### State Reducers Explained

```python
# Example of how state reduction works
initial_state = {"messages": [HumanMessage("Hello")]}

# Node 1 returns
node1_output = {"messages": [AIMessage("I'll help you")]}
# State becomes: {"messages": [HumanMessage("Hello"), AIMessage("I'll help you")]}

# Node 2 returns  
node2_output = {"messages": [ToolMessage("Search results...")]}
# State becomes: {"messages": [HumanMessage("Hello"), AIMessage("I'll help you"), ToolMessage("Search results...")]}

# Without operator.add, each node would overwrite the previous messages!
```

### Nodes: The Action Centers

Nodes are functions that:
- Receive the current state
- Perform some action (LLM call, tool execution, data processing)
- Return updates to the state

```python
def call_openai(state: AgentState):
    """
    Node that calls the LLM to generate a response.
    
    This node:
    1. Takes the current conversation messages from state
    2. Adds system message if configured
    3. Calls the LLM
    4. Returns the LLM's response to be added to state
    
    Parameters:
        state: Current agent state containing conversation history
        
    Returns:
        dict: Update to state containing the LLM's response
    """
    
    messages = state['messages']
    
    # Add system message for context (if configured)
    if self.system:
        messages = [SystemMessage(content=self.system)] + messages
    
    # Call the LLM with conversation history
    # The model is bound with tools, so it can decide to call tools
    message = self.model.invoke(messages)
    
    # Return state update - this message will be appended to messages list
    return {'messages': [message]}

def take_action(state: AgentState):
    """
    Node that executes tool calls requested by the LLM.
    
    This node:
    1. Extracts tool calls from the last LLM message
    2. Executes each tool call
    3. Handles errors (like invalid tool names)
    4. Returns tool results to be added to conversation
    
    Parameters:
        state: Current agent state
        
    Returns:
        dict: Update to state containing tool execution results
    """
    
    # Get the last message (should be from LLM with tool calls)
    last_message = state['messages'][-1]
    tool_calls = last_message.tool_calls
    
    results = []
    
    # Execute each tool call
    for tool_call in tool_calls:
        print(f"Calling: {tool_call}")
        
        # Error handling: Check if tool exists
        if tool_call['name'] not in self.tools:
            print("\n ....bad tool name....")
            result = "bad tool name, retry"  # Instruct LLM to retry
        else:
            # Execute the tool with provided arguments
            tool = self.tools[tool_call['name']]
            result = tool.invoke(tool_call['args'])
        
        # Create tool message to add to conversation
        tool_message = ToolMessage(
            tool_call_id=tool_call['id'],
            name=tool_call['name'], 
            content=str(result)
        )
        results.append(tool_message)
    
    print("Back to the model!")
    return {'messages': results}
```

### Edges: The Flow Control

Edges define how execution moves between nodes. There are two types:

#### 1. Unconditional Edges

```python
# Simple edge: Always go from "action" node to "llm" node
graph.add_edge("action", "llm")

# This means: After executing tools, always return to the LLM
# The LLM can then:
# - Analyze the tool results
# - Call more tools if needed
# - Provide a final answer
```

#### 2. Conditional Edges

```python
def exists_action(state: AgentState):
    """
    Conditional function that determines the next step based on current state.
    
    This function checks if the LLM wants to call any tools.
    
    Parameters:
        state: Current agent state
        
    Returns:
        bool: True if LLM made tool calls, False if ready to end
    """
    
    # Get the last message (LLM's response)
    result = state['messages'][-1]
    
    # Check if the LLM made any tool calls
    return len(result.tool_calls) > 0

# Add conditional edge
graph.add_conditional_edges(
    "llm",                    # From node: "llm" 
    exists_action,            # Condition function
    {                         # Mapping of condition results to next nodes
        True: "action",       # If LLM made tool calls ‚Üí go to action node
        False: END            # If no tool calls ‚Üí end the conversation
    }
)
```

#### Conditional Edges Flow Example

```python
# Flow 1: LLM decides to call a tool
user_input = "What's the weather in SF?"

# 1. llm node: LLM decides it needs to search for weather
# 2. exists_action() returns True (tool calls exist)
# 3. Flow goes to "action" node
# 4. action node: Execute weather search
# 5. Unconditional edge back to "llm" node
# 6. llm node: LLM analyzes weather results and provides answer
# 7. exists_action() returns False (no more tool calls needed)
# 8. Flow goes to END

# Flow 2: LLM can answer directly
user_input = "Hello, how are you?"

# 1. llm node: LLM provides greeting response
# 2. exists_action() returns False (no tools needed)
# 3. Flow goes to END immediately
```

### Advanced Conditional Logic

```python
def advanced_routing(state: AgentState):
    """
    More sophisticated routing based on multiple factors.
    
    This shows how you can implement complex decision logic
    for different types of agent behaviors.
    """
    
    last_message = state['messages'][-1]
    
    # Check what type of response the LLM provided
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        # LLM wants to use tools
        
        # You could add logic here to:
        # - Limit number of tool calls
        # - Route to different action handlers based on tool type
        # - Implement approval workflows for certain tools
        
        return "execute_tools"
    
    elif "search" in last_message.content.lower():
        # LLM mentioned search but didn't call tool - might need guidance
        return "search_guidance"
    
    elif len(state['messages']) > 10:
        # Conversation is getting long - maybe summarize
        return "summarize"
    
    else:
        # Standard completion
        return "end"

# This enables much more sophisticated agent behaviors
graph.add_conditional_edges(
    "llm",
    advanced_routing,
    {
        "execute_tools": "action",
        "search_guidance": "search_helper", 
        "summarize": "summarizer",
        "end": END
    }
)
```

---

## Building Your First Agent

### Step-by-Step Agent Construction

```python
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage

class SearchAgent:
    """
    A complete search agent that can answer questions by searching the web.
    
    This agent demonstrates all core LangGraph concepts:
    - State management
    - Node implementation  
    - Conditional routing
    - Tool integration
    - Error handling
    """
    
    def __init__(self, model, tools, system_prompt=""):
        """
        Initialize the search agent.
        
        Parameters:
            model: ChatOpenAI model instance
            tools: List of tools the agent can use
            system_prompt: Instructions for the agent's behavior
        """
        
        self.system = system_prompt
        self.tools = {tool.name: tool for tool in tools}
        
        # Bind tools to the model so it knows what's available
        self.model = model.bind_tools(tools)
        
        # Create the state graph
        graph = StateGraph(AgentState)
        
        # Add nodes
        graph.add_node("llm", self.call_openai)
        graph.add_node("action", self.take_action)
        
        # Add edges
        graph.add_conditional_edges(
            "llm",                           # From llm node
            self.exists_action,              # Check if tools needed
            {True: "action", False: END}     # Route accordingly
        )
        
        graph.add_edge("action", "llm")      # After tools, back to LLM
        
        # Set the starting point
        graph.set_entry_point("llm")
        
        # Compile the graph
        self.graph = graph.compile()
        
        print("ü§ñ Search Agent initialized successfully!")
        print(f"üìö Available tools: {list(self.tools.keys())}")
    
    def exists_action(self, state: AgentState):
        """Determine if the LLM wants to call tools."""
        result = state['messages'][-1]
        return len(result.tool_calls) > 0
    
    def call_openai(self, state: AgentState):
        """Node that calls the LLM."""
        messages = state['messages']
        
        # Add system prompt if configured
        if self.system:
            messages = [SystemMessage(content=self.system)] + messages
        
        # Get LLM response
        message = self.model.invoke(messages)
        
        return {'messages': [message]}
    
    def take_action(self, state: AgentState):
        """Node that executes tool calls."""
        tool_calls = state['messages'][-1].tool_calls
        results = []
        
        for tool_call in tool_calls:
            print(f"üîß Executing: {tool_call['name']}")
            print(f"üìù Arguments: {tool_call['args']}")
            
            # Validate tool exists
            if tool_call['name'] not in self.tools:
                print("‚ùå Invalid tool name - instructing LLM to retry")
                result = "bad tool name, retry"
            else:
                # Execute the tool
                try:
                    tool = self.tools[tool_call['name']]
                    result = tool.invoke(tool_call['args'])
                    print(f"‚úÖ Tool executed successfully")
                except Exception as e:
                    print(f"‚ùå Tool execution failed: {e}")
                    result = f"Tool execution failed: {e}"
            
            # Create tool message
            tool_message = ToolMessage(
                tool_call_id=tool_call['id'],
                name=tool_call['name'],
                content=str(result)
            )
            results.append(tool_message)
        
        return {'messages': results}
    
    def run(self, user_input: str):
        """
        Run the agent with user input.
        
        Parameters:
            user_input: The user's question or request
            
        Returns:
            str: The agent's final response
        """
        
        print(f"üéØ User input: {user_input}")
        print("üöÄ Starting agent execution...\n")
        
        # Create initial message
        messages = [HumanMessage(content=user_input)]
        
        # Execute the graph
        result = self.graph.invoke({"messages": messages})
        
        # Return the final response
        final_response = result['messages'][-1].content
        print(f"‚ú® Final response: {final_response}")
        
        return final_response

# Create and test the agent
def create_search_agent():
    """Factory function to create a configured search agent."""
    
    # System prompt defines the agent's behavior
    system_prompt = """You are a smart research assistant. Use the search engine to look up information.

Key Instructions:
- You are allowed to make multiple calls (either together or in sequence)
- Only look up information when you are sure of what you want
- If you need to look up some information before asking a follow up question, you are allowed to do that!
- Provide comprehensive answers based on your search results
- If you cannot find information, clearly state what you could not find"""
    
    # Initialize model and tools
    model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    search_tool = TavilySearchResults(max_results=4)
    
    # Create the agent
    agent = SearchAgent(
        model=model,
        tools=[search_tool],
        system_prompt=system_prompt
    )
    
    return agent

# Example usage
agent = create_search_agent()

# Simple single search
response1 = agent.run("What is the weather in San Francisco?")

# Complex multi-search query  
response2 = agent.run("What is the weather in SF and LA? Compare them.")

# Multi-step reasoning query
response3 = agent.run("Who won the super bowl in 2024? In what state is the winning team headquarters located? What is the GDP of that state?")
```

### Understanding Agent Execution Flow

Let's trace through how the agent handles the complex multi-step query:

```python
def trace_agent_execution():
    """
    Demonstrate step-by-step execution of a complex query.
    
    Query: "Who won the super bowl in 2024? In what state is the winning team 
           headquarters located? What is the GDP of that state?"
    """
    
    print("üìã AGENT EXECUTION TRACE")
    print("="*50)
    
    query = """Who won the super bowl in 2024? In what state is the winning team 
               headquarters located? What is the GDP of that state?"""
    
    print(f"User Query: {query}\n")
    
    # Step 1: Initial LLM Call
    print("STEP 1: LLM analyzes the query")
    print("- LLM identifies this requires multiple pieces of information")
    print("- Decides to start with Super Bowl winner search")
    print("- exists_action() returns True ‚Üí go to action node\n")
    
    # Step 2: First Tool Call
    print("STEP 2: Execute first search")
    print("- Tool: tavily_search_results_json")
    print("- Query: '2024 Super Bowl winner'")
    print("- Result: Kansas City Chiefs")
    print("- Returns to LLM node\n")
    
    # Step 3: LLM analyzes first result
    print("STEP 3: LLM analyzes first search result")
    print("- Now knows: Kansas City Chiefs won")
    print("- Realizes needs to find where Chiefs are located")
    print("- Makes second search call")
    print("- exists_action() returns True ‚Üí go to action node\n")
    
    # Step 4: Second Tool Call
    print("STEP 4: Execute second search")
    print("- Tool: tavily_search_results_json")
    print("- Query: 'Kansas City Chiefs headquarters state'")
    print("- Result: Missouri")
    print("- Returns to LLM node\n")
    
    # Step 5: LLM analyzes second result
    print("STEP 5: LLM analyzes second search result")
    print("- Now knows: Chiefs are in Missouri")
    print("- Realizes needs Missouri GDP data")
    print("- Makes third search call")
    print("- exists_action() returns True ‚Üí go to action node\n")
    
    # Step 6: Third Tool Call
    print("STEP 6: Execute third search")
    print("- Tool: tavily_search_results_json")
    print("- Query: 'Missouri GDP 2024'")
    print("- Result: Approximately $460.7 billion")
    print("- Returns to LLM node\n")
    
    # Step 7: Final LLM response
    print("STEP 7: LLM provides comprehensive answer")
    print("- Has all required information")
    print("- Synthesizes into structured response")
    print("- exists_action() returns False ‚Üí END")
    print("\nFinal Answer:")
    print("1. The Kansas City Chiefs won the Super Bowl in 2024.")
    print("2. The Kansas City Chiefs are headquartered in Kansas City, Missouri.")
    print("3. Missouri's GDP in 2024 was approximately $460.7 billion.")

# Run the trace
trace_agent_execution()
```

---

## Advanced Agent Patterns

### Parallel Tool Execution

Some queries can benefit from parallel tool execution:

```python
class ParallelSearchAgent(SearchAgent):
    """
    Enhanced agent that can execute multiple tools in parallel.
    
    This is useful when you need multiple independent pieces of information
    that don't depend on each other.
    """
    
    def call_openai(self, state: AgentState):
        """Enhanced LLM node that can suggest parallel execution."""
        messages = state['messages']
        
        if self.system:
            messages = [SystemMessage(content=self.system)] + messages
        
        message = self.model.invoke(messages)
        
        # Check if this is a parallel-suitable query
        if self._should_parallelize(message):
            print("üîÑ LLM suggested parallel tool execution")
        
        return {'messages': [message]}
    
    def _should_parallelize(self, message):
        """
        Determine if the LLM's tool calls can be executed in parallel.
        
        Parallel execution is suitable when:
        - Multiple tool calls are made
        - Tool calls don't depend on each other's results
        """
        
        if not hasattr(message, 'tool_calls') or len(message.tool_calls) < 2:
            return False
        
        # Simple heuristic: if all tools are searches with different queries
        tool_names = [call['name'] for call in message.tool_calls]
        search_tools = ['tavily_search_results_json', 'web_search', 'google_search']
        
        return all(name in search_tools for name in tool_names)

# Example of parallel execution
def demonstrate_parallel_execution():
    """Show how parallel tool execution works."""
    
    print("üîÑ PARALLEL EXECUTION EXAMPLE")
    print("="*40)
    
    query = "What is the weather in SF and LA?"
    print(f"Query: {query}\n")
    
    print("Traditional Sequential Execution:")
    print("1. Search for SF weather (2 seconds)")
    print("2. Wait for result")
    print("3. Search for LA weather (2 seconds)")
    print("4. Wait for result")
    print("Total time: ~4 seconds\n")
    
    print("Parallel Execution:")
    print("1. Start SF weather search")
    print("2. Start LA weather search (simultaneously)")
    print("3. Wait for both results")
    print("Total time: ~2 seconds")
    
    # The LLM can make both calls in a single response:
    example_llm_response = """
    I need to search for weather in both cities. I'll search for both simultaneously.
    
    Tool calls:
    1. tavily_search_results_json(query="weather in San Francisco")
    2. tavily_search_results_json(query="weather in Los Angeles")
    """
    
    print(f"\nLLM Response Strategy:\n{example_llm_response}")

demonstrate_parallel_execution()
```

### Error Recovery and Retries

```python
class RobustAgent(SearchAgent):
    """
    Agent with enhanced error handling and recovery mechanisms.
    """
    
    def __init__(self, model, tools, system_prompt="", max_retries=3):
        super().__init__(model, tools, system_prompt)
        self.max_retries = max_retries
        self.retry_count = 0
    
    def take_action(self, state: AgentState):
        """Enhanced action node with retry logic."""
        tool_calls = state['messages'][-1].tool_calls
        results = []
        
        for tool_call in tool_calls:
            result = self._execute_tool_with_retry(tool_call)
            
            tool_message = ToolMessage(
                tool_call_id=tool_call['id'],
                name=tool_call['name'],
                content=str(result)
            )
            results.append(tool_message)
        
        return {'messages': results}
    
    def _execute_tool_with_retry(self, tool_call):
        """Execute a tool with retry logic."""
        
        for attempt in range(self.max_retries):
            try:
                if tool_call['name'] not in self.tools:
                    return f"Error: Tool '{tool_call['name']}' not available. Available tools: {list(self.tools.keys())}"
                
                tool = self.tools[tool_call['name']]
                result = tool.invoke(tool_call['args'])
                
                print(f"‚úÖ Tool executed successfully on attempt {attempt + 1}")
                return result
                
            except Exception as e:
                print(f"‚ùå Attempt {attempt + 1} failed: {e}")
                
                if attempt == self.max_retries - 1:
                    return f"Error: Tool execution failed after {self.max_retries} attempts. Last error: {e}"
                
                # Add delay between retries
                import time
                time.sleep(2 ** attempt)  # Exponential backoff
        
        return "Error: Maximum retries exceeded"
```

### Agent with Memory and Context

```python
class ContextualAgent(SearchAgent):
    """
    Agent that maintains context across multiple conversations.
    """
    
    def __init__(self, model, tools, system_prompt=""):
        super().__init__(model, tools, system_prompt)
        self.conversation_history = []
        self.context_cache = {}
    
    def call_openai(self, state: AgentState):
        """Enhanced LLM call with conversation context."""
        messages = state['messages']
        
        # Add relevant context from previous conversations
        if self.conversation_history:
            context_prompt = self._build_context_prompt()
            if context_prompt:
                messages = [SystemMessage(content=context_prompt)] + messages
        
        if self.system:
            system_msg = SystemMessage(content=self.system)
            messages = [system_msg] + messages
        
        message = self.model.invoke(messages)
        return {'messages': [message]}
    
    def _build_context_prompt(self):
        """Build context from previous conversations."""
        if not self.conversation_history:
            return ""
        
        recent_context = self.conversation_history[-3:]  # Last 3 conversations
        
        context_parts = ["Previous conversation context:"]
        for i, conv in enumerate(recent_context, 1):
            context_parts.append(f"{i}. Q: {conv['question'][:100]}...")
            context_parts.append(f"   A: {conv['answer'][:100]}...")
        
        return "\n".join(context_parts)
    
    def run(self, user_input: str):
        """Run agent and save conversation to context."""
        response = super().run(user_input)
        
        # Save to conversation history
        self.conversation_history.append({
            'question': user_input,
            'answer': response,
            'timestamp': datetime.now()
        })
        
        return response
```

---

## Multi-Action Execution

### How LLMs Execute Multiple Actions

The power of LangGraph agents lies in their ability to execute multiple actions before providing a final answer. Here's how this works:

#### Single vs. Multi-Action Comparison

```python
def compare_execution_patterns():
    """
    Compare single-action vs multi-action execution patterns.
    """
    
    print("üîç EXECUTION PATTERN COMPARISON")
    print("="*50)
    
    # Single Action Pattern
    print("SINGLE ACTION (Traditional Chat):")
    print("User: 'What's the weather in SF?'")
    print("‚Üí LLM: Calls search tool once")
    print("‚Üí Returns: Weather information")
    print("Total actions: 1\n")
    
    # Multi-Action Pattern
    print("MULTI-ACTION (Agent):")
    print("User: 'Compare weather in SF and LA, and tell me which is better for outdoor activities'")
    print("‚Üí Action 1: Search SF weather")
    print("‚Üí Action 2: Search LA weather") 
    print("‚Üí Action 3: Search outdoor activity recommendations")
    print("‚Üí Analysis: Compare all information")
    print("‚Üí Returns: Comprehensive comparison with recommendations")
    print("Total actions: 3+\n")
    
    # Complex Multi-Step Pattern
    print("COMPLEX MULTI-STEP (Advanced Agent):")
    print("User: 'Plan a trip to the city with the best weather this weekend'")
    print("‚Üí Action 1: Search weekend weather forecasts for multiple cities")
    print("‚Üí Action 2: Search travel costs to top weather destinations")
    print("‚Üí Action 3: Search accommodation availability")
    print("‚Üí Action 4: Search local attractions and activities")
    print("‚Üí Analysis: Weigh weather, cost, availability, and activities")
    print("‚Üí Returns: Complete trip plan with justification")
    print("Total actions: 4+")

compare_execution_patterns()
```

#### The Decision Loop

```python
def explain_decision_loop():
    """
    Explain how the agent's decision loop enables multi-action execution.
    """
    
    print("üîÑ AGENT DECISION LOOP")
    print("="*30)
    
    loop_steps = [
        {
            "step": "Analyze Current State",
            "description": "LLM examines conversation history and available information",
            "decision": "What do I know? What do I still need to find out?"
        },
        {
            "step": "Plan Next Action", 
            "description": "LLM decides what tool to call or whether to provide final answer",
            "decision": "Do I have enough information, or do I need to search/calculate more?"
        },
        {
            "step": "Execute Action",
            "description": "If tool needed, execute it and add results to conversation",
            "decision": "Tool executed successfully, what did I learn?"
        },
        {
            "step": "Evaluate Results",
            "description": "LLM analyzes tool results and updates its understanding", 
            "decision": "Does this answer the question, or do I need more information?"
        },
        {
            "step": "Continue or Conclude",
            "description": "LLM decides whether to continue with more actions or provide final answer",
            "decision": "Loop back to step 1, or provide final comprehensive response?"
        }
    ]
    
    for i, step in enumerate(loop_steps, 1):
        print(f"\n{i}. {step['step']}:")
        print(f"   What happens: {step['description']}")
        print(f"   LLM thinks: \"{step['decision']}\"")
    
    print(f"\nüîÅ This loop continues until the LLM determines it has")
    print(f"   sufficient information to provide a complete answer.")

explain_decision_loop()
```

#### Practical Multi-Action Examples

```python
class MultiActionExamples:
    """Examples demonstrating different multi-action patterns."""
    
    @staticmethod
    def sequential_dependency_example():
        """
        Example where each action depends on the previous one.
        
        This shows how agents can build up knowledge step by step.
        """
        
        print("üìä SEQUENTIAL DEPENDENCY PATTERN")
        print("="*40)
        
        query = "What's the stock price of the company that makes the iPhone?"
        
        print(f"Query: {query}\n")
        print("Agent reasoning and actions:")
        print("1. 'I need to find who makes the iPhone'")
        print("   ‚Üí Search: 'iPhone manufacturer'")
        print("   ‚Üí Result: Apple Inc.")
        print()
        print("2. 'Now I need Apple's stock symbol'")
        print("   ‚Üí Search: 'Apple Inc stock symbol'") 
        print("   ‚Üí Result: AAPL")
        print()
        print("3. 'Now I can get the current stock price'")
        print("   ‚Üí Search: 'AAPL current stock price'")
        print("   ‚Üí Result: $182.52")
        print()
        print("4. 'I have all the information needed'")
        print("   ‚Üí Final answer: Apple Inc. (AAPL) is currently trading at $182.52")
        
        print("\nüí° Key insight: Each search depends on the previous result")
    
    @staticmethod
    def parallel_gathering_example():
        """
        Example where multiple independent pieces of information are gathered.
        """
        
        print("üîÄ PARALLEL GATHERING PATTERN")
        print("="*40)
        
        query = "Compare the weather, population, and cost of living between San Francisco and New York"
        
        print(f"Query: {query}\n")
        print("Agent can gather information in parallel:")
        print("Batch 1 - Weather Information:")
        print("  ‚Üí Search: 'San Francisco weather today'")
        print("  ‚Üí Search: 'New York weather today'")
        print()
        print("Batch 2 - Population Information:")
        print("  ‚Üí Search: 'San Francisco population 2024'")
        print("  ‚Üí Search: 'New York population 2024'")
        print()
        print("Batch 3 - Cost of Living:")
        print("  ‚Üí Search: 'San Francisco cost of living index'")
        print("  ‚Üí Search: 'New York cost of living index'")
        print()
        print("Final Analysis:")
        print("  ‚Üí Compare all gathered data")
        print("  ‚Üí Provide comprehensive comparison")
        
        print("\nüí° Key insight: Independent searches can be batched together")
    
    @staticmethod
    def adaptive_refinement_example():
        """
        Example where the agent adapts its search strategy based on results.
        """
        
        print("üéØ ADAPTIVE REFINEMENT PATTERN")
        print("="*40)
        
        query = "Find me a good restaurant recommendation"
        
        print(f"Query: {query}\n")
        print("Agent adapts based on results quality:")
        print("1. Initial search: 'good restaurants'")
        print("   ‚Üí Result: Too generic, need more specific criteria")
        print()
        print("2. Refined search: 'highly rated restaurants near me'")
        print("   ‚Üí Result: Still need location context")
        print()
        print("3. Context gathering: 'current location services'")
        print("   ‚Üí Result: No location available, ask user")
        print()
        print("4. Alternative approach: 'popular restaurant types 2024'")
        print("   ‚Üí Result: Italian, Japanese, Farm-to-table trending")
        print()
        print("5. Targeted search: 'best Italian restaurants reviews'")
        print("   ‚Üí Result: Specific recommendations with ratings")
        print()
        print("6. Final recommendation with reasoning")
        
        print("\nüí° Key insight: Agent adapts strategy when initial approach doesn't work")

# Run the examples
examples = MultiActionExamples()
examples.sequential_dependency_example()
print("\n" + "="*60 + "\n")
examples.parallel_gathering_example()
print("\n" + "="*60 + "\n")
examples.adaptive_refinement_example()
```

#### Advanced Multi-Action Orchestration

```python
class AdvancedMultiActionAgent(SearchAgent):
    """
    Advanced agent with sophisticated multi-action orchestration capabilities.
    """
    
    def __init__(self, model, tools, system_prompt="", max_actions_per_cycle=5):
        super().__init__(model, tools, system_prompt)
        self.max_actions_per_cycle = max_actions_per_cycle
        self.action_history = []
    
    def enhanced_system_prompt(self):
        """Enhanced system prompt for multi-action awareness."""
        return f"""{self.system}

MULTI-ACTION CAPABILITIES:
- You can execute up to {self.max_actions_per_cycle} tool calls in a single response
- Use parallel execution when searches are independent
- Use sequential execution when results depend on each other
- Always explain your action strategy before executing tools
- Provide comprehensive analysis after gathering all information

ACTION STRATEGIES:
1. PARALLEL: When you need independent pieces of information
   Example: Weather in multiple cities, prices of different products
   
2. SEQUENTIAL: When each action depends on previous results
   Example: Find company ‚Üí get stock symbol ‚Üí get current price
   
3. ADAPTIVE: When you need to refine your search based on results
   Example: Broad search ‚Üí analyze results ‚Üí targeted follow-up search

Remember to think step-by-step and explain your reasoning!"""
    
    def call_openai(self, state: AgentState):
        """Enhanced LLM call with multi-action awareness."""
        messages = state['messages']
        
        # Use enhanced system prompt
        enhanced_prompt = self.enhanced_system_prompt()
        if enhanced_prompt:
            messages = [SystemMessage(content=enhanced_prompt)] + messages
        
        # Add action history context if available
        if self.action_history:
            context = self._build_action_context()
            if context:
                context_msg = SystemMessage(content=f"Previous actions in this session: {context}")
                messages = [context_msg] + messages
        
        message = self.model.invoke(messages)
        
        # Log the action plan if tools are called
        if hasattr(message, 'tool_calls') and message.tool_calls:
            self._log_action_plan(message)
        
        return {'messages': [message]}
    
    def _log_action_plan(self, message):
        """Log the LLM's action plan for analysis."""
        action_plan = {
            'timestamp': datetime.now(),
            'tool_calls': len(message.tool_calls),
            'tools_used': [call['name'] for call in message.tool_calls],
            'strategy': self._infer_strategy(message.tool_calls)
        }
        
        self.action_history.append(action_plan)
        
        print(f"üìã Action Plan: {action_plan['strategy']}")
        print(f"üîß Tools to execute: {action_plan['tools_used']}")
    
    def _infer_strategy(self, tool_calls):
        """Infer the execution strategy from tool calls."""
        if len(tool_calls) == 1:
            return "SINGLE_ACTION"
        elif len(tool_calls) > 1:
            # Check if all tools are the same type (likely parallel)
            tool_names = [call['name'] for call in tool_calls]
            if len(set(tool_names)) == 1:
                return "PARALLEL_EXECUTION"
            else:
                return "MIXED_EXECUTION"
        return "UNKNOWN"
    
    def _build_action_context(self):
        """Build context from recent actions."""
        if not self.action_history:
            return ""
        
        recent_actions = self.action_history[-3:]  # Last 3 action plans
        context_parts = []
        
        for i, action in enumerate(recent_actions, 1):
            context_parts.append(f"Action {i}: {action['strategy']} using {action['tools_used']}")
        
        return "; ".join(context_parts)

def create_advanced_agent():
    """Create an advanced multi-action agent."""
    
    system_prompt = """You are an advanced research assistant with sophisticated multi-action capabilities.

CORE PRINCIPLES:
1. Always think before acting - explain your strategy
2. Use the most efficient execution pattern for each query
3. Provide comprehensive, well-reasoned responses
4. Learn from previous actions to improve subsequent ones

EXECUTION PATTERNS:
- For comparisons: Use parallel searches
- For dependent information: Use sequential searches  
- For broad topics: Start broad, then narrow down
- For specific facts: Direct targeted search"""
    
    model = ChatOpenAI(model="gpt-4o", temperature=0)  # More capable model for complex reasoning
    search_tool = TavilySearchResults(max_results=4)
    
    agent = AdvancedMultiActionAgent(
        model=model,
        tools=[search_tool],
        system_prompt=system_prompt,
        max_actions_per_cycle=5
    )
    
    return agent

# Test the advanced agent
def test_multi_action_scenarios():
    """Test various multi-action scenarios."""
    
    agent = create_advanced_agent()
    
    test_scenarios = [
        {
            "name": "Parallel Information Gathering",
            "query": "Compare the GDP, population, and capital cities of France, Germany, and Italy",
            "expected_pattern": "PARALLEL_EXECUTION"
        },
        {
            "name": "Sequential Dependency",
            "query": "What's the current stock price of the company that owns ChatGPT?",
            "expected_pattern": "SEQUENTIAL_EXECUTION"  
        },
        {
            "name": "Adaptive Research",
            "query": "Find me the best programming language to learn for AI development in 2024",
            "expected_pattern": "ADAPTIVE_REFINEMENT"
        }
    ]
    
    for scenario in test_scenarios:
        print(f"\nüß™ Testing: {scenario['name']}")
        print(f"üìù Query: {scenario['query']}")
        print(f"üéØ Expected pattern: {scenario['expected_pattern']}")
        print("-" * 60)
        
        try:
            response = agent.run(scenario['query'])
            print(f"‚úÖ Scenario completed successfully")
        except Exception as e:
            print(f"‚ùå Scenario failed: {e}")

# Run the tests
# test_multi_action_scenarios()
```

---

## Production Considerations

### Performance Optimization

```python
class ProductionAgent(AdvancedMultiActionAgent):
    """
    Production-ready agent with performance optimizations and monitoring.
    """
    
    def __init__(self, model, tools, system_prompt="", **kwargs):
        super().__init__(model, tools, system_prompt, **kwargs)
        
        # Performance tracking
        self.performance_metrics = {
            'total_executions': 0,
            'total_tool_calls': 0,
            'average_response_time': 0,
            'error_count': 0,
            'cache_hits': 0
        }
        
        # Response caching
        self.response_cache = {}
        self.cache_ttl = kwargs.get('cache_ttl', 3600)  # 1 hour default
        
        # Rate limiting
        self.rate_limiter = {
            'calls_per_minute': kwargs.get('rate_limit', 60),
            'call_history': []
        }
    
    def run(self, user_input: str, use_cache=True):
        """Enhanced run method with caching and performance monitoring."""
        
        start_time = time.time()
        
        try:
            # Check rate limiting
            if not self._check_rate_limit():
                return "Rate limit exceeded. Please try again later."
            
            # Check cache if enabled
            if use_cache:
                cached_response = self._get_cached_response(user_input)
                if cached_response:
                    self.performance_metrics['cache_hits'] += 1
                    return cached_response
            
            # Execute the agent
            response = super().run(user_input)
            
            # Cache the response
            if use_cache:
                self._cache_response(user_input, response)
            
            # Update performance metrics
            execution_time = time.time() - start_time
            self._update_performance_metrics(execution_time, success=True)
            
            return response
            
        except Exception as e:
            self.performance_metrics['error_count'] += 1
            self._update_performance_metrics(time.time() - start_time, success=False)
            
            # Log error for monitoring
            print(f"‚ùå Agent execution failed: {e}")
            return f"I apologize, but I encountered an error: {str(e)}"
    
    def _check_rate_limit(self):
        """Check if request is within rate limits."""
        import time
        
        current_time = time.time()
        minute_ago = current_time - 60
        
        # Remove old calls
        self.rate_limiter['call_history'] = [
            call_time for call_time in self.rate_limiter['call_history']
            if call_time > minute_ago
        ]
        
        # Check if under limit
        if len(self.rate_limiter['call_history']) >= self.rate_limiter['calls_per_minute']:
            return False
        
        # Add current call
        self.rate_limiter['call_history'].append(current_time)
        return True
    
    def _get_cached_response(self, user_input):
        """Retrieve cached response if available and not expired."""
        import hashlib
        import time
        
        # Create cache key
        cache_key = hashlib.md5(user_input.encode()).hexdigest()
        
        if cache_key in self.response_cache:
            cached_data = self.response_cache[cache_key]
            
            # Check if cache is still valid
            if time.time() - cached_data['timestamp'] < self.cache_ttl:
                print("üíæ Retrieved response from cache")
                return cached_data['response']
            else:
                # Remove expired cache entry
                del self.response_cache[cache_key]
        
        return None
    
    def _cache_response(self, user_input, response):
        """Cache the response for future use."""
        import hashlib
        import time
        
        cache_key = hashlib.md5(user_input.encode()).hexdigest()
        
        self.response_cache[cache_key] = {
            'response': response,
            'timestamp': time.time()
        }
        
        # Limit cache size (keep only 100 most recent)
        if len(self.response_cache) > 100:
            oldest_key = min(self.response_cache.keys(), 
                           key=lambda k: self.response_cache[k]['timestamp'])
            del self.response_cache[oldest_key]
    
    def _update_performance_metrics(self, execution_time, success=True):
        """Update performance tracking metrics."""
        self.performance_metrics['total_executions'] += 1
        
        # Update average response time
        current_avg = self.performance_metrics['average_response_time']
        total_execs = self.performance_metrics['total_executions']
        
        new_avg = ((current_avg * (total_execs - 1)) + execution_time) / total_execs
        self.performance_metrics['average_response_time'] = new_avg
        
        if not success:
            self.performance_metrics['error_count'] += 1
    
    def get_performance_report(self):
        """Generate a performance report."""
        metrics = self.performance_metrics
        
        report = f"""
üèÜ AGENT PERFORMANCE REPORT
================================
Total Executions: {metrics['total_executions']}
Total Tool Calls: {metrics['total_tool_calls']}
Average Response Time: {metrics['average_response_time']:.2f}s
Cache Hits: {metrics['cache_hits']}
Error Count: {metrics['error_count']}
Success Rate: {((metrics['total_executions'] - metrics['error_count']) / max(1, metrics['total_executions']) * 100):.1f}%
Cache Hit Rate: {(metrics['cache_hits'] / max(1, metrics['total_executions']) * 100):.1f}%
"""
        
        return report
```

### Error Handling and Graceful Degradation

```python
class ResilientAgent(ProductionAgent):
    """
    Agent with comprehensive error handling and graceful degradation.
    """
    
    def __init__(self, model, tools, system_prompt="", **kwargs):
        super().__init__(model, tools, system_prompt, **kwargs)
        
        # Fallback configurations
        self.fallback_model = kwargs.get('fallback_model')
        self.max_retries = kwargs.get('max_retries', 3)
        self.fallback_responses = {
            'search_unavailable': "I apologize, but my search capabilities are currently unavailable. Please try again later or ask me something I can answer from my training data.",
            'model_error': "I'm experiencing technical difficulties. Let me try to help you with a simpler approach.",
            'tool_error': "Some of my tools are unavailable, but I'll do my best to help with the information I have."
        }
    
    def take_action(self, state: AgentState):
        """Enhanced action execution with comprehensive error handling."""
        tool_calls = state['messages'][-1].tool_calls
        results = []
        
        for tool_call in tool_calls:
            result = self._execute_tool_safely(tool_call)
            
            tool_message = ToolMessage(
                tool_call_id=tool_call['id'],
                name=tool_call['name'],
                content=str(result)
            )
            results.append(tool_message)
        
        return {'messages': results}
    
    def _execute_tool_safely(self, tool_call):
        """Execute tool with comprehensive error handling and fallbacks."""
        
        for attempt in range(self.max_retries):
            try:
                # Validate tool exists
                if tool_call['name'] not in self.tools:
                    return self._handle_missing_tool(tool_call)
                
                # Validate tool arguments
                validated_args = self._validate_tool_args(tool_call)
                
                # Execute tool
                tool = self.tools[tool_call['name']]
                result = tool.invoke(validated_args)
                
                # Validate result
                if self._is_valid_result(result):
                    return result
                else:
                    return self._handle_invalid_result(tool_call, result)
                
            except Exception as e:
                print(f"‚ùå Tool execution attempt {attempt + 1} failed: {e}")
                
                if attempt == self.max_retries - 1:
                    return self._handle_tool_failure(tool_call, e)
                
                # Wait before retry with exponential backoff
                time.sleep(2 ** attempt)
        
        return self._handle_tool_failure(tool_call, "Maximum retries exceeded")
    
    def _handle_missing_tool(self, tool_call):
        """Handle missing tool gracefully."""
        available_tools = list(self.tools.keys())
        
        return f"""Tool '{tool_call['name']}' is not available. 
Available tools: {available_tools}
Please try rephrasing your request or ask me to use a different approach."""
    
    def _validate_tool_args(self, tool_call):
        """Validate and sanitize tool arguments."""
        args = tool_call.get('args', {})
        
        # Basic validation for search tools
        if tool_call['name'] in ['tavily_search_results_json', 'web_search']:
            if 'query' not in args or not args['query'].strip():
                args['query'] = 'general information'  # Fallback query
            
            # Limit query length
            if len(args['query']) > 200:
                args['query'] = args['query'][:200]
        
        return args
    
    def _is_valid_result(self, result):
        """Check if tool result is valid and useful."""
        if result is None:
            return False
        
        result_str = str(result)
        
        # Check for common error indicators
        error_indicators = ['error', 'failed', 'unavailable', 'timeout']
        if any(indicator in result_str.lower() for indicator in error_indicators):
            return False
        
        # Check for minimum content length
        if len(result_str.strip()) < 10:
            return False
        
        return True
    
    def _handle_invalid_result(self, tool_call, result):
        """Handle cases where tool returns invalid or poor results."""
        return f"The search for '{tool_call.get('args', {}).get('query', 'information')}' returned limited results. Let me try a different approach or provide information from my knowledge base."
    
    def _handle_tool_failure(self, tool_call, error):
        """Handle complete tool failure."""
        tool_name = tool_call['name']
        
        if 'search' in tool_name.lower():
            return self.fallback_responses['search_unavailable']
        else:
            return f"Tool '{tool_name}' is currently unavailable: {error}"
    
    def call_openai(self, state: AgentState):
        """Enhanced LLM call with fallback model support."""
        try:
            return super().call_openai(state)
        except Exception as e:
            print(f"‚ùå Primary model failed: {e}")
            
            if self.fallback_model:
                print("üîÑ Attempting fallback model...")
                try:
                    # Temporarily switch to fallback model
                    original_model = self.model
                    self.model = self.fallback_model.bind_tools(list(self.tools.values()))
                    
                    result = super().call_openai(state)
                    
                    # Restore original model
                    self.model = original_model
                    
                    return result
                    
                except Exception as fallback_error:
                    print(f"‚ùå Fallback model also failed: {fallback_error}")
                    
                    # Return error message as AI response
                    from langchain_core.messages import AIMessage
                    error_message = AIMessage(content=self.fallback_responses['model_error'])
                    return {'messages': [error_message]}
            else:
                # No fallback available
                from langchain_core.messages import AIMessage
                error_message = AIMessage(content=self.fallback_responses['model_error'])
                return {'messages': [error_message]}

def create_production_ready_agent():
    """Create a production-ready agent with all safety features."""
    
    system_prompt = """You are a production AI assistant designed for reliability and robustness.

OPERATIONAL GUIDELINES:
- Always acknowledge when tools are unavailable
- Provide helpful alternatives when primary approaches fail
- Be transparent about limitations and uncertainties
- Prioritize user experience even when facing technical issues
- Use fallback strategies when needed

ERROR RECOVERY:
- If search fails, offer to help with general knowledge
- If results are poor, acknowledge limitations
- If tools are unavailable, suggest alternative approaches
- Always remain helpful and professional"""
    
    # Primary model
    primary_model = ChatOpenAI(model="gpt-4o", temperature=0)
    
    # Fallback model (cheaper/more reliable)
    fallback_model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    
    # Tools
    search_tool = TavilySearchResults(max_results=4)
    
    agent = ResilientAgent(
        model=primary_model,
        tools=[search_tool],
        system_prompt=system_prompt,
        fallback_model=fallback_model,
        max_retries=3,
        cache_ttl=1800,  # 30 minutes
        rate_limit=100   # 100 calls per minute
    )
    
    return agent

# Example usage and testing
def test_production_agent():
    """Test the production agent with various scenarios."""
    
    agent = create_production_ready_agent()
    
    test_cases = [
        "What's the weather in San Francisco?",  # Normal case
        "",  # Empty input
        "Search for information about a very specific technical topic that might not exist",  # Difficult search
        "Compare the economies of 15 different countries",  # High complexity
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nüß™ Test Case {i}: {test_case}")
        print("-" * 50)
        
        try:
            response = agent.run(test_case)
            print(f"‚úÖ Response: {response[:200]}...")
        except Exception as e:
            print(f"‚ùå Failed: {e}")
    
    # Print performance report
    print("\n" + agent.get_performance_report())

# Run production test
# test_production_agent()
```

### Deployment and Monitoring

```python
class MonitoredAgent(ResilientAgent):
    """
    Agent with comprehensive monitoring and logging for production deployment.
    """
    
    def __init__(self, model, tools, system_prompt="", **kwargs):
        super().__init__(model, tools, system_prompt, **kwargs)
        
        # Monitoring configuration
        self.monitoring_enabled = kwargs.get('monitoring_enabled', True)
        self.log_level = kwargs.get('log_level', 'INFO')
        
        # Health check configuration
        self.health_check_interval = kwargs.get('health_check_interval', 300)  # 5 minutes
        self.last_health_check = time.time()
        
        # Setup logging
        self._setup_logging()
    
    def _setup_logging(self):
        """Setup comprehensive logging for production monitoring."""
        import logging
        
        # Create logger
        self.logger = logging.getLogger('langgraph_agent')
        self.logger.setLevel(getattr(logging, self.log_level))
        
        # Create handler if it doesn't exist
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
    
    def run(self, user_input: str, session_id=None, **kwargs):
        """Enhanced run method with comprehensive monitoring."""
        
        if self.monitoring_enabled:
            self.logger.info(f"Agent execution started - Session: {session_id}")
            self.logger.debug(f"User input: {user_input}")
        
        # Perform health check if needed
        if time.time() - self.last_health_check > self.health_check_interval:
            self._perform_health_check()
        
        start_time = time.time()
        
        try:
            response = super().run(user_input, **kwargs)
            
            execution_time = time.time() - start_time
            
            if self.monitoring_enabled:
                self.logger.info(f"Agent execution completed - Time: {execution_time:.2f}s - Session: {session_id}")
                self.logger.debug(f"Response length: {len(response)} characters")
            
            return response
            
        except Exception as e:
            execution_time = time.time() - start_time
            
            if self.monitoring_enabled:
                self.logger.error(f"Agent execution failed - Time: {execution_time:.2f}s - Error: {e} - Session: {session_id}")
            
            raise
    
    def _perform_health_check(self):
        """Perform system health check."""
        self.last_health_check = time.time()
        
        health_status = {
            'timestamp': time.time(),
            'model_available': True,
            'tools_available': True,
            'memory_usage': 'normal',
            'performance': 'good'
        }
        
        try:
            # Test model availability
            test_message = [HumanMessage(content="Health check")]
            self.model.invoke(test_message)
            
        except Exception as e:
            health_status['model_available'] = False
            self.logger.warning(f"Model health check failed: {e}")
        
        try:
            # Test tool availability
            for tool_name, tool in self.tools.items():
                # Perform lightweight test if possible
                pass
                
        except Exception as e:
            health_status['tools_available'] = False
            self.logger.warning(f"Tools health check failed: {e}")
        
        # Log health status
        if self.monitoring_enabled:
            status = "HEALTHY" if all([health_status['model_available'], health_status['tools_available']]) else "DEGRADED"
            self.logger.info(f"Health check completed - Status: {status}")
        
        return health_status
    
    def get_monitoring_dashboard(self):
        """Generate a monitoring dashboard summary."""
        
        performance_report = self.get_performance_report()
        health_status = self._perform_health_check()
        
        dashboard = f"""
üîç AGENT MONITORING DASHBOARD
============================

{performance_report}

HEALTH STATUS:
- Model Available: {'‚úÖ' if health_status['model_available'] else '‚ùå'}
- Tools Available: {'‚úÖ' if health_status['tools_available'] else '‚ùå'}
- Memory Usage: {health_status['memory_usage']}
- Performance: {health_status['performance']}

SYSTEM INFO:
- Cache Size: {len(self.response_cache)} entries
- Rate Limit: {self.rate_limiter['calls_per_minute']} calls/minute
- Monitoring: {'‚úÖ Enabled' if self.monitoring_enabled else '‚ùå Disabled'}
- Last Health Check: {time.ctime(self.last_health_check)}
"""
        
        return dashboard

# Create a fully monitored production agent
def create_monitored_production_agent():
    """Create a fully monitored production agent."""
    
    system_prompt = """You are a production AI assistant with comprehensive monitoring and error handling.

PRODUCTION GUIDELINES:
- Maintain high availability and reliability
- Provide consistent, high-quality responses
- Handle errors gracefully with helpful fallbacks
- Monitor performance and report issues
- Optimize for user experience and system stability"""
    
    primary_model = ChatOpenAI(model="gpt-4o", temperature=0)
    fallback_model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    search_tool = TavilySearchResults(max_results=4)
    
    agent = MonitoredAgent(
        model=primary_model,
        tools=[search_tool],
        system_prompt=system_prompt,
        fallback_model=fallback_model,
        monitoring_enabled=True,
        log_level='INFO',
        health_check_interval=300,
        cache_ttl=1800,
        rate_limit=100
    )
    
    print("üöÄ Production agent deployed successfully!")
    print(agent.get_monitoring_dashboard())
    
    return agent

# Deploy the production agent
# production_agent = create_monitored_production_agent()
```

## Summary

This comprehensive manual has covered:

1. **Core LangGraph Concepts**: Understanding state, nodes, edges, and conditional routing
2. **Multi-Action Execution**: How agents can perform complex, multi-step reasoning
3. **Production Implementation**: Error handling, monitoring, and deployment strategies
4. **Advanced Patterns**: Parallel execution, adaptive refinement, and contextual awareness

### Key Takeaways

- **Agents are more powerful than simple chat**: They can execute multiple actions and make decisions about what to do next
- **State management is crucial**: The shared state allows information to flow through the entire agent workflow
- **Conditional edges enable intelligence**: Agents can adapt their behavior based on current context and results
- **Production readiness requires robust error handling**: Graceful degradation and monitoring are essential
- **Multi-action patterns unlock complex capabilities**: Sequential dependencies, parallel execution, and adaptive refinement enable sophisticated AI behaviors

LangGraph agents represent a significant advancement in AI application development, enabling the creation of intelligent systems that can handle complex, multi-step tasks with reliability and sophistication.